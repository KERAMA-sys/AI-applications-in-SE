# Done by Presley Oluoch
#  SETUP ENVIRONMENT 
# Install essential packages quietly
!pip install pandas scikit-learn seaborn matplotlib --quiet

# Import required libraries
import pandas as pd                     # For data manipulation
from sklearn.datasets import load_breast_cancer  # Dataset loader
from sklearn.ensemble import RandomForestClassifier  # Our ML model
from sklearn.model_selection import train_test_split  # Data splitting
from sklearn.metrics import (accuracy_score, confusion_matrix, 
                            classification_report)  # Evaluation metrics
import seaborn as sns                  # Visualization
import matplotlib.pyplot as plt        # Plotting

print("âœ… Environment setup complete! All packages imported successfully.")

# LOAD AND EXPLORE DATA 
# Load the built-in breast cancer dataset
cancer_data = load_breast_cancer()

# Convert to pandas DataFrame for easier handling
df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)

# Add target variable and create priority labels
# Note: Original target is 0=malignant, 1=benign
df['diagnosis'] = cancer_data.target  # Original medical labels
df['priority'] = df['diagnosis'].map({0: 'High', 1: 'Low'})  # Our business labels

# Display dataset info
print("\n Dataset Overview:")
print(f"Total samples: {len(df)}")
print(f"Features available: {len(cancer_data.feature_names)}")
print("\n First 5 rows:")
display(df.head())

# Check class distribution
print("\n Class Distribution:")
print(df['priority'].value_counts())

#  DATA VISUALIZATION 
# Set style for plots
sns.set_style("whitegrid")

# Plot class distribution
plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
df['priority'].value_counts().plot(kind='bar', color=['salmon', 'lightgreen'])
plt.title("Priority Class Distribution")
plt.xlabel("Priority Level")
plt.ylabel("Count")

# Plot feature correlation (top 5 features)
plt.subplot(1,2,2)
top_features = ['worst area', 'worst concave points', 'mean concave points', 
               'worst radius', 'mean concavity']
sns.heatmap(df[top_features].corr(), annot=True, cmap="coolwarm")
plt.title("Top Feature Correlations")
plt.tight_layout()
plt.show()

#  PREPARE DATA FOR MODELING 
# Separate features (X) and target (y)
X = df.drop(['diagnosis', 'priority'], axis=1)  # All medical features
y = df['priority']  # Our target labels

# Split data into training (80%) and testing (20%) sets
# random_state ensures reproducible results
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y  # Maintains class distribution
)

print("\n Data Split Results:")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")
print(f"Feature count: {X_train.shape[1]}")

#  MODEL TRAINING 
# Initialize Random Forest Classifier
# n_estimators = number of decision trees in the forest
model = RandomForestClassifier(
    n_estimators=100,  # 100 trees
    random_state=42,    # For reproducibility
    class_weight='balanced'  # Handles class imbalance
)

# Train the model on our training data
model.fit(X_train, y_train)

print("\n Model Training Complete!")
print(f"Model parameters:\n{model.get_params()}")

#  MODEL EVALUATION 
# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Generate classification report
report = classification_report(y_test, y_pred)

# Display results
print("\n Evaluation Metrics:")
print(f"Overall Accuracy: {accuracy:.2%}")
print("\nDetailed Report:")
print(report)

# Confusion matrix visualization
plt.figure(figsize=(6,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['High', 'Low'],
            yticklabels=['High', 'Low'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#  FEATURE IMPORTANCE ANALYSIS 
# Get feature importances and convert to DataFrame
importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# Display top 10 features
print("\n Top 10 Important Features:")
display(importances.head(10))

# Visualize feature importance
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=importances.head(10), palette='viridis')
plt.title('Top 10 Predictive Features')
plt.xlabel('Relative Importance')
plt.tight_layout()
plt.show()

#  PREDICTION DEMO 
#  sample test case (using median values from dataset)
sample_case = pd.DataFrame([df.median(numeric_only=True)], columns=X.columns)

# Make prediction
prediction = model.predict(sample_case)
probabilities = model.predict_proba(sample_case)

print("\n Sample Prediction:")
print(f"Predicted Priority: {prediction[0]}")
print(f"Confidence: {probabilities[0].max():.2%}")
print("\nFeature Values Used:")
display(sample_case[top_features])
